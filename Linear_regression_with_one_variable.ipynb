{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie\n",
    "\n",
    "Przed rozpoczęciem pracy z notatnikiem proszę zmienić jego nazwę dodając na początku numer albumu, imię i nazwisko.\n",
    "{nr_albumu}\\_{imię}\\_{nazwisko}\\_{nazwa}\n",
    "\n",
    "Po wykonaniu wszystkich zadań proszę przesłać wypełniony notatnik przez platformę ELF za pomocą formularza \"Prześlij projekt\" w odpowiedniej sekcji. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja liniowa prosta\n",
    "\n",
    "Regresja liniowa prosta, to szczególny przypadek regresji liniowej, w którym zmienną objaśnaną przewidujemy za pomocą jednej zmiennej objaśniającej. Zadanie będzie polegało na wyznaczeniu funkcji regresji opisującej zależność zarobków od lat doświadczenia. \n",
    "\n",
    "Zbiór danych do tego zadania, to Salary.csv. Znajduje się w katalogu datasets.\n",
    "W zbiorze danych znajduje się 35 obserwacji. Każdy wpis jest osobną obserwacją. W zbiorze znajdują się 3 kolumny: YearsExperience, Age i Salary. W pierwszym zadaniu należy wykorzystać YearsExperience i Salary, pomijając Age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "\n",
    "Wczytaj dane z pliku Salary.csv, a następnie stwórz wykres przedstawiający obserwacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fb288a8bb0>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('datasets/Salary.csv', sep=',')\n",
    "x = df['YearsExperience'].values.reshape(df['YearsExperience'].shape[0], 1)\n",
    "y = df['Salary'].values.reshape(df['Salary'].shape[0], 1)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "plt.plot(x,y,'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2\n",
    "\n",
    "Implementacja algorytmu regresji liniowej prostej.\n",
    "\n",
    "Żeby dobrze zrozumieć zapis matematyczny, który początkowo może sprawiać problemy, przejdziemy po kolei po elementach składowych algorytmu. Następnie złączymy elementy w całość.\n",
    "\n",
    "Wzór na regresję liniową w naszym przypadku będzie wyglądał następująco:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x^{(i)}) = \\beta_{0} + \\beta_{1}x_1 = \\beta_{0} + \\beta_{1}  YearsExperience\n",
    "\\end{equation}\n",
    "\n",
    "Przypomnijmy, że zapis $x^{(i)}$ oznacza wektor dla $i$-tej obserwacji. W naszym przypadku ten wektor będzie zawierał tylko $1$ wartość dla cechy $YearsExperience$.\n",
    "\n",
    "\n",
    "_Uwaga: W różnych źródłach algorytm regresji liniowej ma różne zapisy. Czasem podawane są wzory w postaci z sumą, czasem w postaci macierzowej. Jest to spowodowane tym, że algorytm można zaimplementować na te dwa sposoby. Łatwiejszym i bardziej intuicyjnym podejściem jest podejście z sumą, która bezpośrednio sugeruje wykokrzystanie pętli w celu iteracji po obserwacjach/cechach. Implementacja z wykorzystaniem macierzy jest zwykle krótksza i \"bardziej elegancka\", ale również bardziej wydajna. Aby dobrze zrozumieć działanie algorytmu, najlepiej jest zaimplementować obie wersje i porównać je ze sobą._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Inicjalizacja współczynników $\\beta$ regresji\n",
    "\n",
    "Pierwszym krokiem jest inicjalizacja współczynników regresji. W przypadku regresji liniowej prostej mamy dwa współczynniki $\\beta_{0}$ i $\\beta_{1}$. Stwórz dwie zmienne będące współczynnikami regresji liniowej prostej i zainicjalizuj je losowymi wartościami z przedziału $(0,1)$.\n",
    "\n",
    "Dodatkowo stwórz zmienną *alpha*, która przyjmie wartość od $(0,1)$. Możesz ustawić ją ręcznie i sprawdzać jak różne wartości mają wpływ na regresję. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3603612758136162 0.35604940996208023 0.01\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from random import random\n",
    "\n",
    "B0 = random()\n",
    "B1 = random()\n",
    "alfa = 0.01\n",
    "print(B0, B1, alfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Obliczenie predykcji\n",
    "\n",
    "Kolejnym krokiem jest obliczenie wartości funkcji regresji dla wszystkich obserwacji w zbiorze danych. Jest to po prostu wstawienie kolejnych wartości pod wzrór regresji.\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\beta_{0} + \\beta_{1}x_1\n",
    "\\end{equation}\n",
    "\n",
    "Można zrobić to z wykorzystaniem operacji na macierzach (wektorach), albo z wykorzystaniem klasycznej iteracji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def f(x, B0, B1):\n",
    "    return B0+B1*x\n",
    "\n",
    "fx = [f(i, B0, B1) for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Obliczenie błędu\n",
    "\n",
    "Obliczenie wartości błędu regresji nie jest konieczne do aktualizacji wag, jednak jest to bardzo cenna informacja czy nasz algorytm działa poprawnie. Wartość błędu nie może rosnąć w kolejnych epokach.\n",
    "\n",
    "Błąd należy obliczyć zgodnie ze wzorem:\n",
    "\n",
    "\\begin{equation}\n",
    "    SSR = \\frac{1}{2m} \\sum_{i=1}^{m}(f(x^{(i)}) - y^{(i)})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.02198749e+09]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def SSR(x,y, B0, B1):\n",
    "    ssr = 0\n",
    "    for xi, yi in zip(x,y):\n",
    "        skladnik = ((f(xi, B0, B1)-yi)**2)\n",
    "        ssr += skladnik\n",
    "    ssr = ssr/(2*len(x))\n",
    "    return ssr\n",
    "print(SSR(x,y, B0, B1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Obliczenie gradientu \n",
    "\n",
    "Żeby obliczyć gradient, należy obliczyć pochodne cząstkowe względem parametrów $\\beta_{0}$ i $\\beta_{1}$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial SSR}{\\partial \\beta_{0}} = \\frac{1}{m} \\sum^{m}_{i=1} (f(x^{(i)}) - y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial SSR}{\\partial \\beta_{1}} = \\frac{1}{m} \\sum^{m}_{i=1} (f(x^{(i)}) - y^{(i)})x_{1}^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "Tutaj ponownie jak wcześniej można wykorzystać operacje na macierzach, lub iteracyjnie obliczyć sumę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-83885.82204702]), array([-640445.03610194])]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def dSSRdB0(x,y, B0, B1):\n",
    "    result = sum([f(xi, B0, B1)-yi for xi,yi in zip(x,y)])\n",
    "    result = result/(len(x))\n",
    "    return result\n",
    "def dSSRdB1(x,y, B0, B1):\n",
    "    result = sum([(f(xi, B0, B1)-yi)*xi for xi,yi in zip(x,y)])\n",
    "    result = result/(len(x))\n",
    "    return result\n",
    "\n",
    "grad = [dSSRdB0(x,y,B0,B1), dSSRdB1(x,y,B0,B1)]\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.5 Aktualizacja współczynników regresji (wag)\n",
    "\n",
    "Po obliczeniu pochodnych cząstkowych należy obliczyć nowe wartości dla współczynników regresji.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_{0} = \\beta_{0} - \\alpha \\frac{\\partial SSR}{\\partial \\beta_{0}} \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\beta_{1} = \\beta_{1} - \\alpha \\frac{\\partial SSR}{\\partial \\beta_{1}} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[839.21858175] [7243.66894277]\n",
      "[7.29060029e+08]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "B0 = B0 - alfa * grad[0]\n",
    "B1 = B0 - alfa * grad[1]\n",
    "\n",
    "print(B0, B1)\n",
    "\n",
    "print(SSR(x,y,B0,B1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Finalna wersja algorytmu\n",
    "\n",
    "Powyższe działania, to wszystkie elementy potrzebne do stworzenia algorytmu regresji liniowej prostej. Jeden cykl takich operacji nazywany jest **epoką**. Idea obliczania współczynników regresji z wykorzystaniem gradientu polega na iteracyjnym aktualizowaniu współczynników do momentu, aż błąd przestanie znacznie się zmieniać. Można również ustawić jakaś stałą ilość epok. W każdej epoce wykorzystuje się ponownie ten sam zestaw danych.\n",
    "\n",
    "Skoro wiadomo już jakie pojedyncze etapy należy wykonać, żeby obliczyć regresję liniową prostą, przyszedł czas na zebranie wszystkiego w jednym miejscu.\n",
    "\n",
    "Proszę zaimplementować funkcję `learn_and_fit(x, y)`, która dla danych wejściowych będzie zwracać współczynniki regresji w każdej z epok. Dodatkowo proszę zwracać również błąd regresji w każdej epoce. Funkcja może być zaimplementowana w dowolny sposób. Może bezpośrednio zawierać wszystkie instrukcje, może korzystać z innych funkcji pomocniczych albo może korzystać z klasy reprezentującą regresję liniową prostą. \n",
    "\n",
    "Na końcu notebooka znajduje się test jednostkowy, który musi przechodzić przy prawidłowej implementacji algorytmu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B0 = [27956.80052209] ; B1 = [8837.30970702]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "input:\n",
    "x - wartości zmiennej objaśniającej YearsExperience dla wszystkich obserwacji\n",
    "y - wartości zmiennej objaśnianej Salary dla wszystkich obserwacji\n",
    "\n",
    "output:\n",
    "b0: [] - lista z współczynnikami beta_0 w każdej z epok\n",
    "b1: [] - lista z współczynnikami beta_1 w każdej z epok\n",
    "error: [] - lista z błędem w każdej epoce\n",
    "'''\n",
    "\n",
    "\n",
    "def learn_and_fit(x, y):\n",
    "    b0 = [random()]\n",
    "    b1 = [random()]\n",
    "    error = [SSR(x,y,b0[0],b1[0])]\n",
    "    \n",
    "    epoch = 3000\n",
    "    # YOUR CODE HERE\n",
    "    for i in range(epoch):\n",
    "        B0 = b0[-1]\n",
    "        B1 = b1[-1]\n",
    "\n",
    "        gradient = [dSSRdB0(x,y,B0,B1), dSSRdB1(x,y,B0,B1)]\n",
    "        alfa = 0.005\n",
    "        B0 = B0 - alfa * gradient[0]\n",
    "        B1 = B1 - alfa * gradient[1]\n",
    "        ssr = SSR(x,y,B0,B1)\n",
    "\n",
    "        b0.append(B0)\n",
    "        b1.append(B1)\n",
    "        error.append(ssr)\n",
    "    \n",
    "    return b0, b1, error\n",
    "\n",
    "wsp = learn_and_fit(x,y)\n",
    "print(f'B0 = {wsp[0][-1]} ; B1 = {wsp[1][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3\n",
    "\n",
    "Do wykresu stworzonego w zadaniu 1 dodaj prostą regresji. Pokaż w formie animacji, jak zmieniała się funkcja regresji na przestrzeni epok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"ro\")\n",
    "line, = ax.plot([], [], \"b-\", lw=2)\n",
    "\n",
    "plt.xlabel('YearsExperience')\n",
    "plt.ylabel('Salary')\n",
    "res = []\n",
    "\n",
    "for i in range(len(wsp[0])):\n",
    "    res.append([f(x[0][0], wsp[0][i], wsp[1][i]), f(x[-1][0], wsp[0][i], wsp[1][i])])\n",
    "\n",
    "def init():\n",
    "    line.set_data([x[0][0], x[-1][0]], [0,0])\n",
    "    return (line,)\n",
    "\n",
    "def update(i):\n",
    "    line.set_ydata(res[i*10])\n",
    "    return (line,)\n",
    "anim = animation.FuncAnimation(fig, update, init_func=init,\n",
    "                               frames=300, interval=50, blit=True)\n",
    "\n",
    "HTML(anim.to_html5_video())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy jednostkowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "\n",
    "class SimpleLinearRegressionTest(unittest.TestCase):\n",
    "    \n",
    "    def test_learn_and_fit(self):\n",
    "        df = pd.read_csv('datasets/Salary.csv', sep=',')\n",
    "        x = df['YearsExperience'].values.reshape(df['YearsExperience'].shape[0], 1)\n",
    "        y = df['Salary'].values.reshape(df['Salary'].shape[0], 1)\n",
    "        \n",
    "        b0, b1, error = learn_and_fit(x, y)\n",
    "        \n",
    "        self.assertTrue(len(b0) > 1)\n",
    "        self.assertTrue(len(b1) > 1)\n",
    "        self.assertTrue(len(b0) == len(b1))\n",
    "        self.assertTrue(all(i >= j for i, j in zip(error, error[1:]))) #Sprawdzenie, czy błędy nie rosną\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
